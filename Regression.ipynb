{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OaNvk9ZJ_wJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Linear Regression:\n",
        "\n",
        "1 What is Simple Linear Regression?\n",
        "\n",
        "Simple Linear Regression is a statistical method used to model the relationship between two variables. It assumes that there is a linear relationship between the independent variable (X) and the dependent variable (Y), which can be represented by the equation Y = mX + c, where m is the slope, and c is the intercept.\n",
        "\n",
        "2 What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "Linearity: There is a linear relationship between the independent and dependent variables.\n",
        "Independence: The residuals (errors) are independent of each other.\n",
        "Homoscedasticity: The variance of errors is constant across all levels of the independent variable.\n",
        "Normality: The residuals are normally distributed.\n",
        "\n",
        "3 What does the coefficient m represent in the equation Y = mX + c?\n",
        "\n",
        "The coefficient m represents the slope of the regression line. It shows the amount of change in Y for each one-unit increase in X. In other words, it indicates the relationship between the independent variable and the dependent variable.\n",
        "\n",
        "4 What does the intercept c represent in the equation Y = mX + c?\n",
        "\n",
        "The intercept c represents the value of Y when the independent variable (X) is zero. It is the point where the regression line crosses the Y-axis.\n",
        "\n",
        "5 How do we calculate the slope m in Simple Linear Regression?\n",
        "\n",
        "The slope m is calculated using the formula:\n",
        "𝑚\n",
        "=\n",
        "∑\n",
        "(\n",
        "𝑋\n",
        "𝑖\n",
        "−\n",
        "𝑋\n",
        "ˉ\n",
        ")\n",
        "(\n",
        "𝑌\n",
        "𝑖\n",
        "−\n",
        "𝑌\n",
        "ˉ\n",
        ")\n",
        "∑\n",
        "(\n",
        "𝑋\n",
        "𝑖\n",
        "−\n",
        "𝑋\n",
        "ˉ\n",
        ")\n",
        "2\n",
        "m=\n",
        "∑(X\n",
        "i\n",
        "​\n",
        " −\n",
        "X\n",
        "ˉ\n",
        " )\n",
        "2\n",
        "\n",
        "∑(X\n",
        "i\n",
        "​\n",
        " −\n",
        "X\n",
        "ˉ\n",
        " )(Y\n",
        "i\n",
        "​\n",
        " −\n",
        "Y\n",
        "ˉ\n",
        " )\n",
        "​\n",
        "\n",
        "where\n",
        "𝑋\n",
        "𝑖\n",
        "X\n",
        "i\n",
        "​\n",
        "  and\n",
        "𝑌\n",
        "𝑖\n",
        "Y\n",
        "i\n",
        "​\n",
        "  are the data points, and\n",
        "𝑋\n",
        "ˉ\n",
        "X\n",
        "ˉ\n",
        "  and\n",
        "𝑌\n",
        "ˉ\n",
        "Y\n",
        "ˉ\n",
        "  are the means of X and Y, respectively.\n",
        "\n",
        "6 What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        "The least squares method minimizes the sum of the squared residuals (the differences between observed and predicted values). It aims to find the best-fitting line by reducing the overall error.\n",
        "\n",
        "7 How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "\n",
        "R² (R-squared) represents the proportion of the variance in the dependent variable that is explained by the independent variable. A value of R² close to 1 indicates a strong linear relationship, while a value close to 0 indicates a weak relationship.\n",
        "\n",
        "Multiple Linear Regression:\n",
        "\n",
        "8 What is Multiple Linear Regression?\n",
        "\n",
        "Multiple Linear Regression is an extension of simple linear regression that models the relationship between two or more independent variables and a dependent variable. The equation takes the form:\n",
        "𝑌\n",
        "=\n",
        "𝑚\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "𝑚\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝑚\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "+\n",
        "𝑐\n",
        "Y=m\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +m\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +⋯+m\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        " +c\n",
        "where\n",
        "𝑚\n",
        "1\n",
        ",\n",
        "𝑚\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑚\n",
        "𝑛\n",
        "m\n",
        "1\n",
        "​\n",
        " ,m\n",
        "2\n",
        "​\n",
        " ,…,m\n",
        "n\n",
        "​\n",
        "  are the slopes for each predictor variable\n",
        "𝑋\n",
        "1\n",
        ",\n",
        "𝑋\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑋\n",
        "𝑛\n",
        "X\n",
        "1\n",
        "​\n",
        " ,X\n",
        "2\n",
        "​\n",
        " ,…,X\n",
        "n\n",
        "​\n",
        " .\n",
        "\n",
        "9 What is the main difference between Simple and Multiple Linear Regression?\n",
        "\n",
        "The key difference is that simple linear regression uses only one independent variable to predict the dependent variable, while multiple linear regression uses two or more independent variables.\n",
        "\n",
        "10 What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        "Linearity: The relationship between the dependent variable and the independent variables is linear.\n",
        "Independence: The residuals are independent.\n",
        "Homoscedasticity: Constant variance of the residuals.\n",
        "No multicollinearity: Independent variables should not be highly correlated with each other.\n",
        "Normality: The residuals should be normally distributed.\n",
        "\n",
        "11 What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "Heteroscedasticity refers to a condition where the variance of the residuals is not constant across all levels of the independent variables. It violates the assumption of homoscedasticity and can lead to inefficient estimates and invalid statistical tests.\n",
        "\n",
        "12 How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "You can address multicollinearity by:\n",
        "Removing highly correlated predictors.\n",
        "Using regularization techniques like Ridge or Lasso regression.\n",
        "Combining correlated variables into a single variable.\n",
        "Using Principal Component Analysis (PCA) to reduce dimensionality.\n",
        "\n",
        "13 What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "Common techniques include:\n",
        "One-Hot Encoding: Creating binary (0/1) variables for each category.\n",
        "Label Encoding: Assigning a unique number to each category.\n",
        "Dummy Variables: Representing categories with multiple binary columns.\n",
        "\n",
        "14 What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "Interaction terms represent the combined effect of two or more predictors on the dependent variable. They allow the model to account for more complex relationships between the variables.\n",
        "\n",
        "15 How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        "In simple linear regression, the intercept represents the value of Y when X is 0. In multiple linear regression, the intercept represents the expected value of Y when all the independent variables are 0.\n",
        "\n",
        "16 What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        "The slope represents the rate of change in the dependent variable for each unit change in the independent variable. In multiple regression, each slope represents the impact of a single predictor variable, holding other variables constant.\n",
        "\n",
        "17 How does the intercept in a regression model provide context for the relationship between variables?\n",
        "\n",
        "The intercept in a regression model represents the baseline value of the dependent variable when all independent variables are zero. It helps provide context, though its meaning may vary depending on the data.\n",
        "\n",
        "18 What are the limitations of using R² as a sole measure of model performance?\n",
        "\n",
        "R² can be misleading because:\n",
        "It always increases when more predictors are added to the model, even if they are irrelevant.\n",
        "It doesn't capture model overfitting or bias.\n",
        "It doesn't provide information on model accuracy.'\n",
        "\n",
        "19 How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "A large standard error indicates that the estimate of the coefficient is imprecise. This could mean that the variable's effect is uncertain, or the sample size may be too small to get a reliable estimate.\n",
        "\n",
        "20 How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "\n",
        "Heteroscedasticity can be identified by plotting residuals against fitted values. If the spread of residuals increases or decreases with the fitted values, it suggests heteroscedasticity. It is important to address because it can lead to inefficient estimates and invalid inference.\n",
        "\n",
        "21 What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "\n",
        "This suggests that the model is overfitting the data. While the model may explain a large portion of the variance, the inclusion of irrelevant predictors is reducing the model's generalizability.\n",
        "\n",
        "22 Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        "Scaling variables ensures that each variable contributes equally to the model. It also prevents variables with larger magnitudes from disproportionately influencing the model and helps with numerical stability.\n",
        "\n",
        "Polynomial Regression:\n",
        "\n",
        "23 What is polynomial regression?\n",
        "\n",
        "Polynomial regression is a type of regression that models the relationship between the dependent variable and the independent variable as an nth-degree polynomial. It’s used when the relationship is not linear.\n",
        "\n",
        "24 How does polynomial regression differ from linear regression?\n",
        "\n",
        "Linear regression models a straight-line relationship, while polynomial regression allows for curvatures in the relationship by fitting higher-degree polynomial equations.\n",
        "\n",
        "25 When is polynomial regression used?\n",
        "\n",
        "Polynomial regression is used when the data shows a curvilinear trend, which cannot be adequately modeled by simple linear regression.\n",
        "\n",
        "26 What is the general equation for polynomial regression?\n",
        "\n",
        "The general equation for polynomial regression is:\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X+β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "\n",
        "27 Can polynomial regression be applied to multiple variables?\n",
        "\n",
        "Yes, polynomial regression can be extended to multiple variables by including higher-degree terms of the independent variables (e.g.,\n",
        "𝑋\n",
        "1\n",
        "2\n",
        ",\n",
        "𝑋\n",
        "2\n",
        "2\n",
        "X\n",
        "1\n",
        "2\n",
        "​\n",
        " ,X\n",
        "2\n",
        "2\n",
        "​\n",
        " ).\n",
        "28 What are the limitations of polynomial regression?\n",
        "\n",
        "Polynomial regression can overfit the data if the degree of the polynomial is too high. It can also lead to instability in predictions for extreme values.\n",
        "\n",
        "29 What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "\n",
        "Methods include:\n",
        "Cross-validation.\n",
        "AIC/BIC criteria.\n",
        "Visual inspection of residuals.\n",
        "\n",
        "30 Why is visualization important in polynomial regression?\n",
        "\n",
        "Visualization helps to identify the degree of the polynomial that fits the data well and to check if the model overfits or underfits the data.\n",
        "\n",
        "31 How is polynomial regression implemented in Python?\n",
        "\n",
        "In Python, polynomial regression can be implemented using libraries like numpy for creating polynomial features and scikit-learn for fitting the model."
      ],
      "metadata": {
        "id": "ow-cG2pjKUs3"
      }
    }
  ]
}